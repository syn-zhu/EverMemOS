"""
Agentic Retrieval å·¥å…·å‡½æ•°

æä¾› LLM å¼•å¯¼çš„å¤šè½®æ£€ç´¢æ‰€éœ€çš„å·¥å…·ï¼š
1. Sufficiency Check: åˆ¤æ–­æ£€ç´¢ç»“æœæ˜¯å¦å……åˆ†
2. Query Refinement: ç”Ÿæˆæ”¹è¿›çš„æŸ¥è¯¢
3. Document Formatting: æ ¼å¼åŒ–æ–‡æ¡£ä¾› LLM ä½¿ç”¨
"""

import json
import asyncio
from pathlib import Path
from typing import List, Tuple, Optional


def load_prompt_template(template_name: str) -> str:
    """
    åŠ è½½ prompt æ¨¡æ¿
    
    Args:
        template_name: æ¨¡æ¿æ–‡ä»¶åï¼ˆå¦‚ "sufficiency_check.txt"ï¼‰
    
    Returns:
        Prompt æ¨¡æ¿å­—ç¬¦ä¸²
    """
    prompt_dir = Path(__file__).parent.parent / "prompts"
    template_path = prompt_dir / template_name
    
    if not template_path.exists():
        raise FileNotFoundError(f"Prompt template not found: {template_path}")
    
    with open(template_path, "r", encoding="utf-8") as f:
        return f.read()


def format_documents_for_llm(
    results: List[Tuple[dict, float]],
    max_docs: int = 10,
    use_episode: bool = True
) -> str:
    """
    æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¾› LLM ä½¿ç”¨
    
    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨ [(doc, score), ...]
        max_docs: æœ€å¤šåŒ…å«çš„æ–‡æ¡£æ•°
        use_episode: True=ä½¿ç”¨ Episode Memoryï¼ŒFalse=ä½¿ç”¨ Event Log
    
    Returns:
        æ ¼å¼åŒ–çš„æ–‡æ¡£å­—ç¬¦ä¸²
    """
    formatted_docs = []
    
    for i, (doc, score) in enumerate(results[:max_docs], start=1):
        subject = doc.get("subject", "N/A")
        
        # ğŸ”¥ æ ¹æ® use_episode å‚æ•°é€‰æ‹©æ ¼å¼
        if use_episode:
            # ä½¿ç”¨ Episode Memory æ ¼å¼ï¼ˆå®Œæ•´å™è¿°ï¼‰
            episode = doc.get("episode", "N/A")
            
            # é™åˆ¶ episode é•¿åº¦ï¼ˆé¿å… prompt è¿‡é•¿ï¼‰
            if len(episode) > 500:
                episode = episode[:500] + "..."
            
            doc_text = (
                f"Document {i}:\n"
                f"  Title: {subject}\n"
                f"  Content: {episode}\n"
            )
            formatted_docs.append(doc_text)
        else:
            # ä½¿ç”¨ Event Log æ ¼å¼ï¼ˆåŸå­äº‹å®ï¼‰
            if doc.get("event_log") and doc["event_log"].get("atomic_fact"):
                event_log = doc["event_log"]
                time_str = event_log.get("time", "N/A")
                atomic_facts = event_log.get("atomic_fact", [])
                
                if isinstance(atomic_facts, list) and atomic_facts:
                    # æ ¼å¼åŒ–ä¸ºï¼šDocument N: æ ‡é¢˜ + æ—¶é—´ + äº‹å®åˆ—è¡¨
                    facts_text = "\n     ".join(atomic_facts[:5])  # æœ€å¤šæ˜¾ç¤º 5 ä¸ª facts
                    if len(atomic_facts) > 5:
                        facts_text += f"\n     ... and {len(atomic_facts) - 5} more facts"
                    
                    doc_text = (
                        f"Document {i}:\n"
                        f"  Title: {subject}\n"
                        f"  Time: {time_str}\n"
                        f"  Facts:\n"
                        f"     {facts_text}\n"
                    )
                    formatted_docs.append(doc_text)
                    continue
            
            # å¦‚æœæ²¡æœ‰ event_logï¼Œå›é€€åˆ° episode
            episode = doc.get("episode", "N/A")
            if len(episode) > 500:
                episode = episode[:500] + "..."
            
            doc_text = (
                f"Document {i}:\n"
                f"  Title: {subject}\n"
                f"  Content: {episode}\n"
            )
            formatted_docs.append(doc_text)
    
    return "\n".join(formatted_docs)


def parse_json_response(response: str) -> dict:
    """
    è§£æ LLM çš„ JSON å“åº”ï¼ˆå¥å£®å¤„ç†ï¼‰
    
    Args:
        response: LLM åŸå§‹å“åº”å­—ç¬¦ä¸²
    
    Returns:
        è§£æåçš„ JSON å­—å…¸
    """
    try:
        # å°è¯•æå– JSONï¼ˆLLM å¯èƒ½åœ¨å‰åæ·»åŠ é¢å¤–æ–‡æœ¬ï¼‰
        start_idx = response.find("{")
        end_idx = response.rfind("}") + 1
        
        if start_idx == -1 or end_idx == 0:
            raise ValueError("No JSON object found in response")
        
        json_str = response[start_idx:end_idx]
        result = json.loads(json_str)
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "is_sufficient" not in result:
            raise ValueError("Missing 'is_sufficient' field")
        
        # è®¾ç½®é»˜è®¤å€¼
        result.setdefault("reasoning", "No reasoning provided")
        result.setdefault("missing_information", [])
        
        return result
    
    except (json.JSONDecodeError, ValueError) as e:
        print(f"  âš ï¸  Failed to parse LLM response: {e}")
        print(f"  Raw response: {response[:200]}...")
        
        # ä¿å®ˆå›é€€ï¼šå‡è®¾å……åˆ†ï¼ˆé¿å…ä¸å¿…è¦çš„ç¬¬äºŒè½®æ£€ç´¢ï¼‰
        return {
            "is_sufficient": True,
            "reasoning": f"Failed to parse: {str(e)}",
            "missing_information": []
        }


def parse_refined_query(response: str, original_query: str) -> str:
    """
    è§£ææ”¹è¿›åçš„æŸ¥è¯¢
    
    Args:
        response: LLM å“åº”
        original_query: åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºå›é€€ï¼‰
    
    Returns:
        æ”¹è¿›åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    refined = response.strip()
    
    # ç§»é™¤å¸¸è§å‰ç¼€
    prefixes = ["Refined Query:", "Output:", "Answer:", "Query:"]
    for prefix in prefixes:
        if refined.startswith(prefix):
            refined = refined[len(prefix):].strip()
    
    # éªŒè¯é•¿åº¦
    if len(refined) < 5 or len(refined) > 300:
        print(f"  âš ï¸  Invalid refined query length ({len(refined)}), using original")
        return original_query
    
    # é¿å…ä¸åŸæŸ¥è¯¢å®Œå…¨ç›¸åŒ
    if refined.lower() == original_query.lower():
        print(f"  âš ï¸  Refined query identical to original, using original")
        return original_query
    
    return refined


async def check_sufficiency(
    query: str,
    results: List[Tuple[dict, float]],
    llm_client,
    llm_config: dict,
    max_docs: int = 10
) -> Tuple[bool, str, List[str]]:
    """
    æ£€æŸ¥æ£€ç´¢ç»“æœæ˜¯å¦å……åˆ†
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        results: æ£€ç´¢ç»“æœï¼ˆTop 10ï¼‰
        llm_client: LLM å®¢æˆ·ç«¯ï¼ˆAsyncOpenAIï¼‰
        llm_config: LLM é…ç½®å­—å…¸
        max_docs: æœ€å¤šè¯„ä¼°çš„æ–‡æ¡£æ•°
    
    Returns:
        (is_sufficient, reasoning, missing_information)
    """
    try:
        # 1. æ ¼å¼åŒ–æ–‡æ¡£ï¼ˆğŸ”¥ ä½¿ç”¨ Episode Memory æ ¼å¼ï¼‰
        retrieved_docs = format_documents_for_llm(
            results, 
            max_docs=max_docs,
            use_episode=True  # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨ Episode Memory
        )
        
        # 2. åŠ è½½ prompt æ¨¡æ¿
        prompt_template = load_prompt_template("sufficiency_check.txt")
        prompt = prompt_template.format(
            query=query,
            retrieved_docs=retrieved_docs
        )
        
        # 3. è°ƒç”¨ LLMï¼ˆæ·»åŠ è¶…æ—¶æ§åˆ¶ï¼‰
        response = await asyncio.wait_for(
            llm_client.chat.completions.create(
                model=llm_config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,  # ä½æ¸©åº¦ï¼Œåˆ¤æ–­æ›´ç¨³å®š
                max_tokens=500,
            ),
            timeout=30.0  # ğŸ”¥ 30 ç§’è¶…æ—¶
        )
        
        result_text = response.choices[0].message.content or ""
        
        # 4. è§£æ JSON å“åº”
        result = parse_json_response(result_text)
        
        return (
            result["is_sufficient"],
            result["reasoning"],
            result.get("missing_information", [])
        )
    
    except asyncio.TimeoutError:
        print(f"  âŒ Sufficiency check timeout (30s)")
        # è¶…æ—¶å›é€€ï¼šå‡è®¾å……åˆ†
        return True, "Timeout: LLM took too long", []
    except Exception as e:
        print(f"  âŒ Sufficiency check failed: {e}")
        import traceback
        traceback.print_exc()
        # ä¿å®ˆå›é€€ï¼šå‡è®¾å……åˆ†
        return True, f"Error: {str(e)}", []


async def generate_refined_query(
    original_query: str,
    results: List[Tuple[dict, float]],
    missing_info: List[str],
    llm_client,
    llm_config: dict,
    max_docs: int = 10
) -> str:
    """
    ç”Ÿæˆæ”¹è¿›çš„æŸ¥è¯¢
    
    Args:
        original_query: åŸå§‹æŸ¥è¯¢
        results: Round 1 æ£€ç´¢ç»“æœï¼ˆTop 10ï¼‰
        missing_info: ç¼ºå¤±çš„ä¿¡æ¯åˆ—è¡¨
        llm_client: LLM å®¢æˆ·ç«¯
        llm_config: LLM é…ç½®
        max_docs: æœ€å¤šä½¿ç”¨çš„æ–‡æ¡£æ•°
    
    Returns:
        æ”¹è¿›åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    try:
        # 1. æ ¼å¼åŒ–æ–‡æ¡£å’Œç¼ºå¤±ä¿¡æ¯ï¼ˆğŸ”¥ ä½¿ç”¨ Episode Memory æ ¼å¼ï¼‰
        retrieved_docs = format_documents_for_llm(
            results, 
            max_docs=max_docs,
            use_episode=True  # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨ Episode Memory
        )
        missing_info_str = ", ".join(missing_info) if missing_info else "N/A"
        
        # 2. åŠ è½½ prompt æ¨¡æ¿
        prompt_template = load_prompt_template("refined_query.txt")
        prompt = prompt_template.format(
            original_query=original_query,
            retrieved_docs=retrieved_docs,
            missing_info=missing_info_str
        )
        
        # 3. è°ƒç”¨ LLMï¼ˆæ·»åŠ è¶…æ—¶æ§åˆ¶ï¼‰
        response = await asyncio.wait_for(
            llm_client.chat.completions.create(
                model=llm_config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # ç¨é«˜æ¸©åº¦ï¼Œå¢åŠ åˆ›é€ æ€§
                max_tokens=150,
            ),
            timeout=30.0  # ğŸ”¥ 30 ç§’è¶…æ—¶
        )
        
        result_text = response.choices[0].message.content or ""
        
        # 4. è§£æå’ŒéªŒè¯
        refined_query = parse_refined_query(result_text, original_query)
        
        return refined_query
    
    except asyncio.TimeoutError:
        print(f"  âŒ Query refinement timeout (30s)")
        # è¶…æ—¶å›é€€ï¼šä½¿ç”¨åŸå§‹æŸ¥è¯¢
        return original_query
    except Exception as e:
        print(f"  âŒ Query refinement failed: {e}")
        import traceback
        traceback.print_exc()
        # å›é€€åˆ°åŸå§‹æŸ¥è¯¢
        return original_query


def parse_multi_query_response(response: str, original_query: str) -> Tuple[List[str], str]:
    """
    è§£æå¤šæŸ¥è¯¢ç”Ÿæˆçš„ JSON å“åº”
    
    Args:
        response: LLM åŸå§‹å“åº”å­—ç¬¦ä¸²
        original_query: åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºå›é€€ï¼‰
    
    Returns:
        (queries_list, reasoning)
    """
    try:
        # æå– JSON
        start_idx = response.find("{")
        end_idx = response.rfind("}") + 1
        
        if start_idx == -1 or end_idx == 0:
            raise ValueError("No JSON object found in response")
        
        json_str = response[start_idx:end_idx]
        result = json.loads(json_str)
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "queries" not in result or not isinstance(result["queries"], list):
            raise ValueError("Missing or invalid 'queries' field")
        
        queries = result["queries"]
        reasoning = result.get("reasoning", "No reasoning provided")
        
        # è¿‡æ»¤å’ŒéªŒè¯æŸ¥è¯¢
        valid_queries = []
        for q in queries:
            if isinstance(q, str) and 5 <= len(q) <= 300:
                # é¿å…ä¸åŸæŸ¥è¯¢å®Œå…¨ç›¸åŒ
                if q.lower().strip() != original_query.lower().strip():
                    valid_queries.append(q.strip())
        
        # è‡³å°‘è¿”å› 1 ä¸ªæŸ¥è¯¢
        if not valid_queries:
            print(f"  âš ï¸  No valid queries generated, using original")
            return [original_query], "Fallback: used original query"
        
        # é™åˆ¶æœ€å¤š 3 ä¸ªæŸ¥è¯¢
        valid_queries = valid_queries[:3]
        
        print(f"  âœ… Generated {len(valid_queries)} valid queries")
        return valid_queries, reasoning
    
    except (json.JSONDecodeError, ValueError) as e:
        print(f"  âš ï¸  Failed to parse multi-query response: {e}")
        print(f"  Raw response: {response[:200]}...")
        
        # å›é€€ï¼šè¿”å›åŸå§‹æŸ¥è¯¢
        return [original_query], f"Parse error: {str(e)}"


async def generate_multi_queries(
    original_query: str,
    results: List[Tuple[dict, float]],
    missing_info: List[str],
    llm_client,
    llm_config: dict,
    max_docs: int = 5,
    num_queries: int = 3
) -> Tuple[List[str], str]:
    """
    ç”Ÿæˆå¤šä¸ªäº’è¡¥çš„æŸ¥è¯¢ï¼ˆç”¨äºå¤šæŸ¥è¯¢æ£€ç´¢ï¼‰
    
    Args:
        original_query: åŸå§‹æŸ¥è¯¢
        results: Round 1 æ£€ç´¢ç»“æœï¼ˆTop 5ï¼‰
        missing_info: ç¼ºå¤±çš„ä¿¡æ¯åˆ—è¡¨
        llm_client: LLM å®¢æˆ·ç«¯
        llm_config: LLM é…ç½®
        max_docs: æœ€å¤šä½¿ç”¨çš„æ–‡æ¡£æ•°ï¼ˆé»˜è®¤ 5ï¼‰
        num_queries: æœŸæœ›ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ 3ï¼Œå®é™…å¯èƒ½æ›´å°‘ï¼‰
    
    Returns:
        (queries_list, reasoning)
        queries_list: ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨ï¼ˆ1-3 ä¸ªï¼‰
        reasoning: LLM çš„ç”Ÿæˆç­–ç•¥è¯´æ˜
    """
    try:
        # 1. æ ¼å¼åŒ–æ–‡æ¡£å’Œç¼ºå¤±ä¿¡æ¯ï¼ˆğŸ”¥ ä½¿ç”¨ Episode Memory æ ¼å¼ï¼‰
        retrieved_docs = format_documents_for_llm(
            results, 
            max_docs=max_docs,
            use_episode=True  # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨ Episode Memory
        )
        missing_info_str = ", ".join(missing_info) if missing_info else "N/A"
        
        # 2. åŠ è½½ prompt æ¨¡æ¿
        prompt_template = load_prompt_template("multi_query_generation.txt")
        prompt = prompt_template.format(
            original_query=original_query,
            retrieved_docs=retrieved_docs,
            missing_info=missing_info_str
        )
        
        # 3. è°ƒç”¨ LLMï¼ˆæ·»åŠ è¶…æ—¶æ§åˆ¶ï¼‰
        response = await asyncio.wait_for(
            llm_client.chat.completions.create(
                model=llm_config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4,  # ç¨é«˜æ¸©åº¦ï¼Œå¢åŠ æŸ¥è¯¢å¤šæ ·æ€§
                max_tokens=300,  # å¢åŠ  token æ•°ä»¥æ”¯æŒå¤šä¸ªæŸ¥è¯¢
            ),
            timeout=30.0  # ğŸ”¥ 30 ç§’è¶…æ—¶
        )
        
        result_text = response.choices[0].message.content or ""
        
        # 4. è§£æå’ŒéªŒè¯
        queries, reasoning = parse_multi_query_response(result_text, original_query)
        
        print(f"  [Multi-Query] Generated {len(queries)} queries:")
        for i, q in enumerate(queries, 1):
            print(f"    Query {i}: {q[:80]}{'...' if len(q) > 80 else ''}")
        print(f"  [Multi-Query] Strategy: {reasoning}")
        
        return queries, reasoning
    
    except asyncio.TimeoutError:
        print(f"  âŒ Multi-query generation timeout (30s)")
        # è¶…æ—¶å›é€€ï¼šä½¿ç”¨åŸå§‹æŸ¥è¯¢
        return [original_query], "Timeout: used original query"
    except Exception as e:
        print(f"  âŒ Multi-query generation failed: {e}")
        import traceback
        traceback.print_exc()
        # å›é€€åˆ°åŸå§‹æŸ¥è¯¢
        return [original_query], f"Error: {str(e)}"

