"""
LLM Judge è¯„ä¼°å™¨

ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤å™¨æ¥è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚

å¯¹é½åˆ° evaluation_archive çš„è¯„ä¼°é€»è¾‘ï¼š
- ä¿ç•™æ¯æ¬¡ run çš„ç‹¬ç«‹åˆ¤æ–­ (judgment_1, judgment_2, judgment_3)
- åˆ†åˆ«è®¡ç®—æ¯æ¬¡ run çš„å‡†ç¡®ç‡
- è¾“å‡º mean å’Œ std
"""
import asyncio
import json
import numpy as np
from typing import List, Dict, Any
from collections import defaultdict
from openai import AsyncOpenAI
from tqdm import tqdm

from evaluation.src.evaluators.base import BaseEvaluator
from evaluation.src.evaluators.registry import register_evaluator
from evaluation.src.core.data_models import AnswerResult, EvaluationResult
from evaluation.src.utils.prompts import get_prompt, format_prompt


@register_evaluator("llm_judge")
class LLMJudge(BaseEvaluator):
    """LLM è¯„åˆ¤å™¨"""
    
    def __init__(self, config: dict):
        super().__init__(config)
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        llm_config = config.get("llm", {})
        self.client = AsyncOpenAI(
            api_key=llm_config.get("api_key"),
            base_url=llm_config.get("base_url", "https://api.openai.com/v1")
        )
        self.model = llm_config.get("model", "gpt-4o-mini")
        self.num_runs = config.get("num_runs", 3)
    
    async def evaluate(
        self, 
        answer_results: List[AnswerResult]
    ) -> EvaluationResult:
        """
        ä½¿ç”¨ LLM è¯„ä¼°ç­”æ¡ˆ
        
        å¯¹é½åˆ° evaluation_archive çš„è¯„ä¼°é€»è¾‘ï¼š
        - ä¿ç•™æ¯æ¬¡ run çš„ç‹¬ç«‹åˆ¤æ–­
        - åˆ†åˆ«è®¡ç®—æ¯æ¬¡ run çš„å‡†ç¡®ç‡
        - è¿”å› mean å’Œ std
        
        Args:
            answer_results: ç­”æ¡ˆç»“æœåˆ—è¡¨
            
        Returns:
            EvaluationResult: è¯„ä¼°ç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"Evaluation: LLM Judge (model={self.model}, runs={self.num_runs})")
        print(f"{'='*60}")
        
        detailed_results = []
        
        # å¹¶å‘è¯„ä¼°æ‰€æœ‰ç­”æ¡ˆ
        semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°
        
        # ğŸ”¥ ä½¿ç”¨ tqdm è¿›åº¦æ¡ï¼ˆå¯¹é½ evaluation_archiveï¼‰
        pbar = tqdm(total=len(answer_results), desc="âš–ï¸  Evaluate Progress", unit="qa")
        
        async def evaluate_single(answer_result: AnswerResult):
            async with semaphore:
                result = await self._evaluate_single_answer(answer_result)
                pbar.update(1)  # æ›´æ–°è¿›åº¦æ¡
                return result
        
        tasks = [evaluate_single(ar) for ar in answer_results]
        results = await asyncio.gather(*tasks)
        
        # å…³é—­è¿›åº¦æ¡
        pbar.close()
        
        # æ”¶é›†ç»“æœ
        for result in results:
            detailed_results.append(result)
        
        # ğŸ”¥ å¯¹é½åˆ° evaluation_archiveï¼šåˆ†åˆ«è®¡ç®—æ¯æ¬¡ run çš„å‡†ç¡®ç‡
        run_scores = []
        category_stats = defaultdict(lambda: {"correct": [0] * self.num_runs, "total": 0})
        
        for i in range(self.num_runs):
            judgment_key = f"judgment_{i+1}"
            correct_count = 0
            total_count = 0
            
            for result in detailed_results:
                llm_judgments = result.get("llm_judgments", {})
                category = result.get("category")
                
                if judgment_key in llm_judgments:
                    total_count += 1
                    if llm_judgments[judgment_key]:
                        correct_count += 1
                        if category is not None:
                            category_stats[category]["correct"][i] += 1
                
                # ç»Ÿè®¡ category æ€»æ•°ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
                if i == 0 and category is not None:
                    category_stats[category]["total"] += 1
            
            if total_count > 0:
                run_accuracy = correct_count / total_count
                run_scores.append(run_accuracy)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_accuracy = np.mean(run_scores) if run_scores else 0.0
        std_accuracy = np.std(run_scores) if run_scores else 0.0
        
        # è®¡ç®—æ¯ä¸ª category çš„å‡†ç¡®ç‡
        category_accuracies = {}
        for category, stats in category_stats.items():
            cat_accuracies = []
            for i in range(self.num_runs):
                if stats["total"] > 0:
                    cat_acc = stats["correct"][i] / stats["total"]
                    cat_accuracies.append(cat_acc)
            
            if cat_accuracies:
                category_accuracies[str(category)] = {
                    "mean": np.mean(cat_accuracies),
                    "std": np.std(cat_accuracies),
                    "individual_runs": cat_accuracies,
                    "total": stats["total"]
                }
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ:")
        print(f"   - æ€»é—®é¢˜æ•°: {len(answer_results)}")
        print(f"   - å¹³å‡å‡†ç¡®ç‡: {mean_accuracy:.4f} ({mean_accuracy*100:.2f}%)")
        print(f"   - æ ‡å‡†å·®: {std_accuracy:.4f}")
        print(f"   - å„æ¬¡ run å‡†ç¡®ç‡: {[f'{s:.4f}' for s in run_scores]}")
        
        if category_accuracies:
            print(f"\nğŸ“Š æŒ‰ Category ç»Ÿè®¡:")
            for cat, stats in sorted(category_accuracies.items()):
                print(f"   Category {cat}: {stats['mean']:.4f} Â± {stats['std']:.4f} (n={stats['total']})")
        
        # ğŸ”¥ å¯¹é½åˆ° evaluation_archiveï¼šæŒ‰ conversation åˆ†ç»„
        grouped_results = self._group_by_conversation(detailed_results)
        
        return EvaluationResult(
            total_questions=len(answer_results),
            correct=int(mean_accuracy * len(answer_results)),  # ä½¿ç”¨ mean è®¡ç®—
            accuracy=mean_accuracy,
            detailed_results=grouped_results,  # â¬…ï¸ ä½¿ç”¨åˆ†ç»„åçš„ç»“æœ
            metadata={
                "model": self.model,
                "num_runs": self.num_runs,
                "mean_accuracy": mean_accuracy,
                "std_accuracy": std_accuracy,
                "run_scores": run_scores,
                "category_accuracies": category_accuracies
            }
        )
    
    def _group_by_conversation(self, detailed_results: List[Dict]) -> Dict[str, List[Dict]]:
        """
        å°†ç»“æœæŒ‰ conversation åˆ†ç»„
        
        å¯¹é½åˆ° evaluation_archive çš„æ ¼å¼ï¼š
        {
            "locomo_exp_user_0": [...],
            "locomo_exp_user_1": [...],
        }
        """
        grouped = defaultdict(list)
        
        for result in detailed_results:
            question_id = result.get("question_id", "")
            
            # ä» question_id æå– conversation ä¿¡æ¯
            # ä¾‹å¦‚: "locomo_0_qa0" -> "locomo_exp_user_0"
            # ä¾‹å¦‚: "personamem_5_qa2" -> "personamem_exp_user_5"
            if "_qa" in question_id:
                parts = question_id.split("_qa")
                conv_id = parts[0]  # "locomo_0" or "personamem_5"
                
                # è½¬æ¢ä¸º evaluation_archive çš„æ ¼å¼
                if "_" in conv_id:
                    dataset_name, conv_num = conv_id.rsplit("_", 1)
                    group_key = f"{dataset_name}_exp_user_{conv_num}"
                else:
                    group_key = f"{conv_id}_exp_user_0"
            else:
                # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„
                group_key = "default_group"
            
            grouped[group_key].append(result)
        
        return dict(grouped)
    
    async def _evaluate_single_answer(self, answer_result: AnswerResult) -> dict:
        """
        è¯„ä¼°å•ä¸ªç­”æ¡ˆ
        
        ğŸ”¥ å¯¹é½åˆ° evaluation_archiveï¼š
        - ä¿ç•™æ¯æ¬¡ run çš„ç‹¬ç«‹åˆ¤æ–­ (judgment_1, judgment_2, judgment_3)
        - ä¸åšå¤šæ•°æŠ•ç¥¨ï¼Œä¸ç”Ÿæˆ is_correct
        """
        question = answer_result.question
        golden_answer = answer_result.golden_answer
        generated_answer = answer_result.answer
        
        # å¤šæ¬¡è¯„ä¼°ï¼Œä¿ç•™ç‹¬ç«‹åˆ¤æ–­
        judgments = []
        for _ in range(self.num_runs):
            is_correct = await self._judge_answer(
                question, golden_answer, generated_answer
            )
            judgments.append(is_correct)
        
        # ğŸ”¥ å¯¹é½åˆ° evaluation_archiveï¼šä½¿ç”¨ judgment_1, judgment_2, ... æ ¼å¼
        llm_judgments = {
            f"judgment_{i+1}": judgment 
            for i, judgment in enumerate(judgments)
        }
        
        return {
            "question_id": answer_result.question_id,
            "question": question,
            "golden_answer": golden_answer,
            "generated_answer": generated_answer,
            "llm_judgments": llm_judgments,  # â¬…ï¸ å¯¹é½æ ¼å¼
            "category": answer_result.category,
        }
    
    async def _judge_answer(
        self, 
        question: str, 
        golden_answer: str, 
        generated_answer: str
    ) -> bool:
        """
        ä½¿ç”¨ LLM åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        
        Returns:
            True å¦‚æœæ­£ç¡®ï¼ŒFalse å¦‚æœé”™è¯¯
        """
        # ä½¿ç”¨é…ç½®åŒ–çš„ prompts
        system_prompt = get_prompt("llm_judge", "system_prompt")
        user_prompt = format_prompt(
            "llm_judge",
            "user_prompt",
            question=question,
            golden_answer=golden_answer,
            generated_answer=generated_answer
        )
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0,
            )
            
            content = response.choices[0].message.content
            result = json.loads(content)
            label = result.get("label", "WRONG")
            
            return label.strip().upper() == "CORRECT"
        
        except Exception as e:
            print(f"  âš ï¸ LLM Judge å¤±è´¥: {e}")
            return False

