import random
import time
from memory_layer.memory_manager import MemorizeRequest, MemorizeOfflineRequest
from memory_layer.memory_manager import MemoryManager
from memory_layer.types import MemoryType, MemCell, Memory, RawDataType
from infra_layer.adapters.out.persistence.document.memory.memcell import DataTypeEnum
from memory_layer.memory_extractor.profile_memory_extractor import (
    ProfileMemory,
    ProfileMemoryExtractor,
    ProfileMemoryExtractRequest,
    ProfileMemoryMerger,
    ProjectInfo,
)
from memory_layer.memory_extractor.group_profile_memory_extractor import (
    GroupProfileMemoryExtractor,
    GroupProfileMemoryExtractRequest,
    GroupProfileMemory,
)
from core.di import get_bean_by_type, enable_mock_mode, scan_packages
from infra_layer.adapters.out.persistence.repository.episodic_memory_raw_repository import (
    EpisodicMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.conversation_status_raw_repository import (
    ConversationStatusRawRepository,
)
from infra_layer.adapters.out.persistence.repository.core_memory_raw_repository import (
    CoreMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.memcell_raw_repository import (
    MemCellRawRepository,
)
from infra_layer.adapters.out.persistence.repository.group_user_profile_memory_raw_repository import (
    GroupUserProfileMemoryRawRepository,
)
from infra_layer.adapters.out.persistence.repository.group_profile_raw_repository import (
    GroupProfileRawRepository,
)
from biz_layer.conversation_data_repo import ConversationDataRepository
from memory_layer.types import RawDataType
from typing import List, Dict, Optional
import uuid
from datetime import datetime, timedelta
import os
import asyncio
from collections import defaultdict
from common_utils.datetime_utils import get_now_with_timezone, to_iso_format
from memory_layer.memcell_extractor.base_memcell_extractor import StatusResult
import traceback

from core.lock.redis_distributed_lock import distributed_lock
from core.observation.logger import get_logger
from infra_layer.adapters.out.search.elasticsearch.converter.episodic_memory_converter import (
    EpisodicMemoryConverter,
)
from infra_layer.adapters.out.search.milvus.converter.episodic_memory_milvus_converter import (
    EpisodicMemoryMilvusConverter,
)
from infra_layer.adapters.out.search.repository.episodic_memory_milvus_repository import (
    EpisodicMemoryMilvusRepository,
)
from biz_layer.memcell_milvus_sync import MemCellMilvusSyncService

logger = get_logger(__name__)


def _convert_data_type_to_raw_data_type(data_type) -> RawDataType:
    """
    将不同的数据类型枚举转换为统一的RawDataType

    Args:
        data_type: 可能是DataTypeEnum、RawDataType或字符串

    Returns:
        RawDataType: 转换后的统一数据类型
    """
    if isinstance(data_type, RawDataType):
        return data_type

    # 获取字符串值
    if hasattr(data_type, 'value'):
        type_str = data_type.value
    else:
        type_str = str(data_type)

    # 映射转换
    type_mapping = {
        "Conversation": RawDataType.CONVERSATION,
        "CONVERSATION": RawDataType.CONVERSATION,
        # 其他类型映射到CONVERSATION作为默认值
    }

    return type_mapping.get(type_str, RawDataType.CONVERSATION)


from biz_layer.mem_db_operations import (
    _convert_timestamp_to_time,
    _convert_episode_memory_to_doc,
    _save_memcell_to_database,
    _save_profile_memory_to_core,
    ConversationStatus,
    _update_status_for_new_conversation,
    _update_status_for_continuing_conversation,
    _update_status_after_memcell_extraction,
    _convert_original_data_for_profile_extractor,
    _save_group_profile_memory,
    _get_user_organization,
    _save_profile_memory_to_group_user_profile_memory,
    _convert_document_to_group_importance_evidence,
    _get_raw_data_by_time_range,
    _normalize_datetime_for_storage,
    _convert_projects_participated_list,
    _convert_group_profile_raw_to_memory_format,
)


def if_memorize(memcells: List[MemCell]) -> bool:
    return True


def extract_message_time(raw_data):
    """
    从RawData对象中提取消息时间

    Args:
        raw_data: RawData对象

    Returns:
        datetime: 消息时间，如果无法提取则返回None
    """
    # 优先从timestamp字段获取
    if hasattr(raw_data, 'timestamp') and raw_data.timestamp:
        try:
            return _normalize_datetime_for_storage(raw_data.timestamp)
        except Exception as e:
            logger.debug(f"Failed to parse timestamp from raw_data.timestamp: {e}")
            pass

    # 从extend字段获取
    if (
        hasattr(raw_data, 'extend')
        and raw_data.extend
        and isinstance(raw_data.extend, dict)
    ):
        timestamp_val = raw_data.extend.get('timestamp')
        if timestamp_val:
            try:
                return _normalize_datetime_for_storage(timestamp_val)
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from extend field: {e}")
                pass

    return None


from core.observation.tracing.decorators import trace_logger


@trace_logger(operation_name="mem_memorize preprocess_conv_request", log_level="info")
async def preprocess_conv_request(
    request: MemorizeRequest, current_time: datetime
) -> MemorizeRequest:

    # load status table， 重新读取部分历史数据，覆盖history_raw_data_list和new_raw_data_list
    logger.info("开始处理状态表逻辑...")

    # 获取Repository实例
    try:

        status_repo = get_bean_by_type(ConversationStatusRawRepository)

        logger.info("成功获取状态表和数据Repository")
    except Exception as e:
        logger.error(f"获取Repository失败，使用原逻辑: {e}")
        traceback.print_exc()
        # 如果无法获取Repository，继续使用原有逻辑
    if not request.new_raw_data_list:
        logger.info("[mem_memorize] 没有新数据需要处理")
        return None
    else:
        # 1. 获取当前对话状态
        # 查询对话状态，真实repository返回DocConversationStatus
        doc_status = await status_repo.get_by_group_id(request.group_id)
        logger.debug(f"[mem_memorize] doc_status: {doc_status}")

        # 转换为业务层模型
        conversation_status = None
        if doc_status:
            conversation_status = ConversationStatus(
                group_id=doc_status.group_id,
                old_msg_start_time=_convert_timestamp_to_time(
                    doc_status.old_msg_start_time
                ),
                new_msg_start_time=_convert_timestamp_to_time(
                    doc_status.new_msg_start_time
                ),
                last_memcell_time=_convert_timestamp_to_time(
                    doc_status.last_memcell_time
                ),
                created_at=(
                    doc_status.created_at.isoformat()
                    if hasattr(doc_status, 'created_at') and doc_status.created_at
                    else to_iso_format(current_time)
                ),
                updated_at=(
                    doc_status.updated_at.isoformat()
                    if hasattr(doc_status, 'updated_at') and doc_status.updated_at
                    else to_iso_format(current_time)
                ),
            )


        # 3. 根据状态表决定如何读取历史数据
        history_raw_data_list = request.history_raw_data_list
        new_raw_data_list = request.new_raw_data_list

        if conversation_status:
            # 存在状态记录，根据状态决定数据范围
            logger.info(f"[mem_memorize] 找到对话状态，重新构建数据范围")

            # 获取old_msg_start_time和new_msg_start_time作为分界点
            old_msg_start_time = _normalize_datetime_for_storage(
                conversation_status.old_msg_start_time
            )
            new_msg_start_time = _normalize_datetime_for_storage(
                conversation_status.new_msg_start_time
            )

            # 检查new_raw_data_list中最早的消息时间，如果比current new_msg_start_time更早，则调整时间边界
            # 这是为了解决kafka输入不完全按顺序的问题
            if request.new_raw_data_list and new_msg_start_time:
                # 找到最早的消息时间
                earliest_new_message_time = None
                for raw_data in request.new_raw_data_list:
                    message_time = extract_message_time(raw_data)
                    if message_time and (
                        earliest_new_message_time is None
                        or message_time < earliest_new_message_time
                    ):
                        earliest_new_message_time = message_time

                # 转换new_msg_start_time为datetime对象并比较
                if earliest_new_message_time:

                    # 如果最早消息时间比当前new_msg_start_time更早，则调整时间边界
                    if (
                        new_msg_start_time
                        and earliest_new_message_time < new_msg_start_time
                    ):
                        logger.debug(
                            f"[mem_memorize] 检测到更早的消息: {earliest_new_message_time} < {new_msg_start_time}"
                        )

                        # 调整new_msg_start_time为最早消息时间
                        new_msg_start_time = earliest_new_message_time

                        # 调整old_msg_start_time为min(原old_msg_start_time, new_msg_start_time - 1ms)
                        new_boundary = earliest_new_message_time - timedelta(
                            milliseconds=1
                        )
                        old_msg_start_time = min(old_msg_start_time, new_boundary)

                        logger.debug(
                            f"[mem_memorize] 时间边界已调整: old_msg_start_time={old_msg_start_time}, new_msg_start_time={new_msg_start_time}"
                        )

                        # 时间边界调整后，更新conversation status表
                        try:
                            update_data = {
                                "old_msg_start_time": _normalize_datetime_for_storage(
                                    old_msg_start_time
                                ),
                                "new_msg_start_time": _normalize_datetime_for_storage(
                                    new_msg_start_time
                                ),
                                "updated_at": current_time,
                            }

                            result = await status_repo.upsert_by_group_id(
                                request.group_id, update_data
                            )
                            if result:
                                logger.debug(
                                    f"[mem_memorize] 时间边界调整后，conversation status表更新成功"
                                )
                            else:
                                logger.debug(
                                    f"[mem_memorize] 时间边界调整后，conversation status表更新失败"
                                )
                        except Exception as e:
                            logger.debug(
                                f"时间边界调整后，更新conversation status表异常: {e}"
                            )

            # 读取历史数据：从old_msg_start_time到new_msg_start_time 前闭后开
            now = time.time()
            history_data = []
            if new_msg_start_time:
                history_data = await _get_raw_data_by_time_range(
                    request.group_id,
                    start_time=_normalize_datetime_for_storage(old_msg_start_time),
                    end_time=_normalize_datetime_for_storage(new_msg_start_time),
                    limit=500,  # 限制历史消息数量
                )
            # 移除高频调试日志
            # 读取新数据：从new_msg_start_time到当前时间 +1ms是为了调整为前闭后闭
            new_data = []
            if new_msg_start_time:
                new_data = await _get_raw_data_by_time_range(
                    request.group_id,
                    start_time=_normalize_datetime_for_storage(new_msg_start_time),
                    end_time=_normalize_datetime_for_storage(current_time)
                    + timedelta(milliseconds=1),  # 添加结束时间为当前时间
                    limit=500,  # 限制新消息数量
                )
            # 移除高频调试日志
            logger.info(
                f"[mem_memorize] 从状态表重新读取: 历史数据 {len(history_data)} 条, 新数据 {len(new_data)} 条"
            )

            # 重新分配数据（如果 Redis 返回空，保留原始数据）
            if history_data or new_data:
                history_raw_data_list = history_data
                new_raw_data_list = new_data
                logger.info(
                    f"[mem_memorize] 使用 Redis 数据: 历史 {len(history_raw_data_list)} 条, 新数据 {len(new_raw_data_list)} 条"
                )
            else:
                history_raw_data_list = request.history_raw_data_list
                new_raw_data_list = request.new_raw_data_list
                logger.info(
                    f"[mem_memorize] Redis 无数据，保留原始请求: 历史 {len(history_raw_data_list)} 条, 新数据 {len(new_raw_data_list)} 条"
                )

        else:
            # 新对话，创建状态记录
            logger.info(f"[mem_memorize] 新对话，创建状态记录")

            # 获取最早消息时间
            earliest_new_time = _convert_timestamp_to_time(current_time, current_time)
            if request.new_raw_data_list:
                first_msg = request.new_raw_data_list[0]
                if hasattr(first_msg, 'content') and isinstance(
                    first_msg.content, dict
                ):
                    earliest_new_time = first_msg.content.get(
                        'timestamp', earliest_new_time
                    )
                elif hasattr(first_msg, 'timestamp'):
                    earliest_new_time = first_msg.timestamp

            # 使用封装函数创建新对话状态
            await _update_status_for_new_conversation(
                status_repo, request, earliest_new_time, current_time
            )
        # 4. 检查是否有数据需要处理
        if not new_raw_data_list:
            logger.info(f"[mem_memorize] 没有新数据需要处理")
            return None

        # 更新request的数据
        request.history_raw_data_list = history_raw_data_list
        request.new_raw_data_list = new_raw_data_list
    return request


async def update_status_when_no_memcell(
    request: MemorizeRequest,
    status_result: StatusResult,
    current_time: datetime,
    data_type: RawDataType,
):
    if data_type == RawDataType.CONVERSATION:
        # 尝试更新状态表
        try:
            status_repo = get_bean_by_type(ConversationStatusRawRepository)

            if status_result.should_wait:
                logger.info(f"[mem_memorize] 判断为无法判断边界继续等待，不更新状态表")
                return
            else:
                logger.info(f"[mem_memorize] 判断为非边界，继续累积msg，更新状态表")
                # 获取最新消息时间戳
                latest_time = _convert_timestamp_to_time(current_time, current_time)
                if request.new_raw_data_list:
                    last_msg = request.new_raw_data_list[-1]
                    if hasattr(last_msg, 'content') and isinstance(
                        last_msg.content, dict
                    ):
                        latest_time = last_msg.content.get('timestamp', latest_time)
                    elif hasattr(last_msg, 'timestamp'):
                        latest_time = last_msg.timestamp

                if not latest_time:
                    latest_time = min(latest_time, current_time)

                # 使用封装函数更新对话延续状态
                await _update_status_for_continuing_conversation(
                    status_repo, request, latest_time, current_time
                )

        except Exception as e:
            logger.error(f"更新状态表失败: {e}")
    else:
        pass


async def update_status_after_memcell(
    request: MemorizeRequest,
    memcells: List[MemCell],
    current_time: datetime,
    data_type: RawDataType,
):
    if data_type == RawDataType.CONVERSATION:
        # 更新状态表中的last_memcell_time至memcells最后一个时间戳
        try:
            status_repo = get_bean_by_type(ConversationStatusRawRepository)

            # 获取MemCell的时间戳
            memcell_time = None
            if memcells and hasattr(memcells[-1], 'timestamp'):
                memcell_time = memcells[-1].timestamp
            else:
                memcell_time = current_time

            # 使用封装函数更新MemCell提取后的状态
            await _update_status_after_memcell_extraction(
                status_repo, request, memcell_time, current_time
            )

            logger.info(f"[mem_memorize] 记忆提取完成，状态表已更新")

        except Exception as e:
            logger.error(f"最终状态表更新失败: {e}")
    else:
        pass


async def save_personal_profile_memory(
    profile_memories: List[ProfileMemory], version: Optional[str] = None
):
    logger.info(f"[mem_memorize] 保存 {len(profile_memories)} 个个人档案记忆到数据库")
    # 初始化Repository实例
    core_memory_repo = get_bean_by_type(CoreMemoryRawRepository)

    # 保存个人档案记忆到GroupUserProfileMemoryRawRepository
    for profile_mem in profile_memories:
        await _save_profile_memory_to_core(profile_mem, core_memory_repo, version)
        # 移除单个操作成功日志


async def save_memories(
    memory_list: List[Memory], current_time: datetime, version: Optional[str] = None
):
    logger.info(f"[mem_memorize] 保存 {len(memory_list)} 个记忆到数据库")
    # 初始化Repository实例
    episodic_memory_repo = get_bean_by_type(EpisodicMemoryRawRepository)
    group_user_profile_memory_repo = get_bean_by_type(
        GroupUserProfileMemoryRawRepository
    )
    group_profile_raw_repo = get_bean_by_type(GroupProfileRawRepository)
    episodic_memory_milvus_repo = get_bean_by_type(EpisodicMemoryMilvusRepository)

    # 按memory_type分类保存
    episode_memories = [
        m for m in memory_list if m.memory_type == MemoryType.EPISODE_SUMMARY
    ]
    profile_memories = [m for m in memory_list if m.memory_type == MemoryType.PROFILE]
    group_profile_memories = [
        m for m in memory_list if m.memory_type == MemoryType.GROUP_PROFILE
    ]

    # 保存情景记忆到 EpisodicMemoryRawRepository（包括 ES/Milvus）
    for episode_mem in episode_memories:
        # 转换为EpisodicMemory文档格式
        doc = _convert_episode_memory_to_doc(episode_mem, current_time)
        doc = await episodic_memory_repo.append_episodic_memory(doc)
        episode_mem.event_id = str(doc.event_id)
        
        # 保存到 ES
        es_doc = EpisodicMemoryConverter.from_mongo(doc)
        await es_doc.save()
        
        # 保存到 Milvus（添加缺失的字段）
        milvus_entity = EpisodicMemoryMilvusConverter.from_mongo(doc)
        vector = milvus_entity.get("vector") if isinstance(milvus_entity, dict) else None
        
        if not vector or (isinstance(vector, list) and len(vector) == 0):
            logger.warning(
                "[mem_memorize] 跳过写入Milvus：向量为空或缺失，event_id=%s",
                getattr(doc, 'event_id', None),
            )
        else:
            # ⚠️ 旧 converter 缺少字段，手动补全
            milvus_entity["memory_sub_type"] = "episode"  # 标记为 episode 类型
            milvus_entity["start_time"] = 0
            milvus_entity["end_time"] = 0
            # 字段名修正：旧 schema 是 detail → 新 schema 是 metadata
            if "detail" in milvus_entity:
                milvus_entity["metadata"] = milvus_entity.pop("detail")
            else:
                milvus_entity["metadata"] = "{}"
            # 确保 search_content 字段存在
            if "search_content" not in milvus_entity:
                milvus_entity["search_content"] = milvus_entity.get("episode", "")[:500]
            await episodic_memory_milvus_repo.insert(milvus_entity)
        
        logger.debug(f"✅ 保存 episode_memory: {episode_mem.event_id}")

    # 保存Profile记忆到CoreMemoryRawRepository
    for profile_mem in profile_memories:
        try:
            await _save_profile_memory_to_group_user_profile_memory(
                profile_mem, group_user_profile_memory_repo, version
            )
        except Exception as e:
            logger.error(f"保存Profile记忆失败: {e}")

    for group_profile_mem in group_profile_memories:
        try:
            await _save_group_profile_memory(
                group_profile_mem, group_profile_raw_repo, version
            )
        except Exception as e:
            logger.error(f"保存Group Profile记忆失败: {e}")

    logger.info(f"[mem_memorize] 保存完成:")
    logger.info(f"  - EPISODE_SUMMARY: {len(episode_memories)} 个")
    logger.info(f"  - PROFILE: {len(profile_memories)} 个")
    logger.info(f"  - GROUP_PROFILE: {len(group_profile_memories)} 个")


async def load_core_memories(
    request: MemorizeRequest, participants: List[str], current_time: datetime
):
    logger.info(f"[mem_memorize] 读取用户数据: {participants}")
    # 初始化Repository实例
    core_memory_repo = get_bean_by_type(CoreMemoryRawRepository)

    # 读取用户CoreMemory数据
    user_core_memories = {}
    for user_id in participants:
        try:
            core_memory = await core_memory_repo.get_by_user_id(user_id)
            if core_memory:
                user_core_memories[user_id] = core_memory
            # 移除单个用户的成功/失败日志
        except Exception as e:
            logger.error(f"获取用户 {user_id} CoreMemory失败: {e}")

    logger.info(f"[mem_memorize] 获取到 {len(user_core_memories)} 个用户CoreMemory")

    # 直接从CoreMemory转换为ProfileMemory对象列表
    old_memory_list = []
    if user_core_memories:
        for user_id, core_memory in user_core_memories.items():
            if core_memory:
                # 直接创建ProfileMemory对象
                profile_memory = ProfileMemory(
                    # Memory 基类必需字段
                    memory_type=MemoryType.CORE,
                    user_id=user_id,
                    timestamp=to_iso_format(current_time),
                    ori_event_id_list=[],
                    # Memory 基类可选字段
                    subject=f"{getattr(core_memory, 'user_name', user_id)}的个人档案",
                    summary=f"用户{user_id}的基本信息：{getattr(core_memory, 'position', '未知角色')}",
                    group_id=request.group_id,
                    participants=[user_id],
                    type=RawDataType.CONVERSATION,
                    # ProfileMemory 特有字段 - 直接使用原始字典格式
                    hard_skills=getattr(core_memory, 'hard_skills', None),
                    soft_skills=getattr(core_memory, 'soft_skills', None),
                    output_reasoning=getattr(core_memory, 'output_reasoning', None),
                    motivation_system=getattr(core_memory, 'motivation_system', None),
                    fear_system=getattr(core_memory, 'fear_system', None),
                    value_system=getattr(core_memory, 'value_system', None),
                    humor_use=getattr(core_memory, 'humor_use', None),
                    colloquialism=getattr(core_memory, 'colloquialism', None),
                    projects_participated=_convert_projects_participated_list(
                        getattr(core_memory, 'projects_participated', None)
                    ),
                )
                old_memory_list.append(profile_memory)

        logger.info(
            f"[mem_memorize] 直接转换了 {len(old_memory_list)} 个CoreMemory为ProfileMemory"
        )
    else:
        logger.info(f"[mem_memorize] 没有用户CoreMemory数据，old_memory_list为空")


async def memorize(request: MemorizeRequest) -> List[Memory]:

    # logger.info(f"[mem_memorize] request: {request}")

    # logger.info(f"[mem_memorize] memorize request: {request}")
    logger.info(f"[mem_memorize] request.current_time: {request.current_time}")
    # 获取当前时间，用于所有时间相关操作
    if request.current_time:
        current_time = request.current_time
    else:
        current_time = get_now_with_timezone() + timedelta(seconds=1)
    logger.info(f"[mem_memorize] 当前时间: {current_time}")

    memory_manager = MemoryManager()

    memory_types = [MemoryType.EPISODE_SUMMARY]
    if request.raw_data_type == RawDataType.CONVERSATION:
        request = await preprocess_conv_request(request, current_time)
        if request == None:
            return None

    if request.raw_data_type == RawDataType.CONVERSATION:
        # async with distributed_lock(f"memcell_extract_{request.group_id}") as acquired:
        #     # 120s等待，获取不到
        #     if not acquired:
        #         logger.warning(f"[mem_memorize] 获取分布式锁失败: {request.group_id}")
        now = time.time()
        logger.debug(
            f"[memorize memorize] 提取MemCell开始: group_id={request.group_id}, group_name={request.group_name}, "
            f"semantic_extraction={request.enable_semantic_extraction}"
        )
        memcell_result = await memory_manager.extract_memcell(
            request.history_raw_data_list,
            request.new_raw_data_list,
            request.raw_data_type,
            request.group_id,
            request.group_name,
            request.user_id_list,
            enable_semantic_extraction=request.enable_semantic_extraction,
            enable_event_log_extraction=request.enable_event_log_extraction,
        )
        logger.debug(f"[memorize memorize] 提取MemCell耗时: {time.time() - now}秒")
    else:
        now = time.time()
        logger.debug(
            f"[memorize memorize] 提取MemCell开始: group_id={request.group_id}, group_name={request.group_name}, "
            f"semantic_extraction={request.enable_semantic_extraction}, "
            f"event_log_extraction={request.enable_event_log_extraction}"
        )
        memcell_result = await memory_manager.extract_memcell(
            request.history_raw_data_list,
            request.new_raw_data_list,
            request.raw_data_type,
            request.group_id,
            request.group_name,
            request.user_id_list,
            enable_semantic_extraction=request.enable_semantic_extraction,
            enable_event_log_extraction=request.enable_event_log_extraction,
        )
        logger.debug(f"[memorize memorize] 提取MemCell耗时: {time.time() - now}秒")

    if memcell_result == None:
        logger.warning(f"[mem_memorize] 跳过提取MemCell")
        return None

    logger.debug(f"[mem_memorize] memcell_result: {memcell_result}")
    memcell, status_result = memcell_result

    if memcell == None:
        await update_status_when_no_memcell(
            request, status_result, current_time, request.raw_data_type
        )
        logger.warning(f"[mem_memorize] 跳过提取MemCell")
        return None
    else:
        logger.info(f"[mem_memorize] 成功提取MemCell")

    # TODO: 读状态表，读取累积的MemCell数据表，判断是否要做memorize计算

    # MemCell存表
    memcell = await _save_memcell_to_database(memcell, current_time)

    # 同步 MemCell 到 Milvus 和 ES（包括 episode/semantic_memories/event_log）
    memcell_repo = get_bean_by_type(MemCellRawRepository)
    doc_memcell = await memcell_repo.get_by_event_id(str(memcell.event_id))
    
    if doc_memcell:
        sync_service = get_bean_by_type(MemCellMilvusSyncService)
        sync_stats = await sync_service.sync_memcell(
            doc_memcell, 
            sync_to_es=True, 
            sync_to_milvus=True
        )
        logger.info(
            f"[mem_memorize] MemCell 同步到 Milvus/ES 完成: {memcell.event_id}, "
            f"stats={sync_stats}"
        )
    else:
        logger.warning(f"[mem_memorize] 无法加载 MemCell 进行同步: {memcell.event_id}")

    # print_memory = random.random() < 0.1

    logger.info(f"[mem_memorize] 成功保存MemCell: {memcell.event_id}")

    # if print_memory:
    #     logger.info(f"[mem_memorize] 打印MemCell: {memcell}")

    memcells = [memcell]

    # 读取记忆的流程
    participants = []
    for memcell in memcells:
        if memcell.participants:
            participants.extend(memcell.participants)

    if if_memorize(memcells):
        # 加锁
        # 使用真实Repository读取用户数据
        old_memory_list = await load_core_memories(request, participants, current_time)

        # 提取记忆
        memory_list = []
        for memory_type in memory_types:
            # 移除单个类型提取的日志
            extracted_memories = await memory_manager.extract_memory(
                memcell_list=memcells,
                memory_type=memory_type,
                user_ids=participants,
                group_id=request.group_id,
                group_name=request.group_name,
                old_memory_list=old_memory_list,
            )
            if extracted_memories:
                memory_list += extracted_memories
        # 移除详细的提取完成日志

        # if print_memory:
        #     logger.info(f"[mem_memorize] 打印记忆: {memory_list}")
        # 保存记忆到数据库
        if memory_list:
            await save_memories(memory_list, current_time)

        await update_status_after_memcell(
            request, memcells, current_time, request.raw_data_type
        )

        # TODO: 实际项目中应该加锁避免并发问题
        # 释放锁
        return memory_list
    else:
        return None



def get_version_from_request(request: MemorizeOfflineRequest) -> str:
    # 1. 获取 memorize_to 日期
    target_date = request.memorize_to

    # 2. 倒退一天
    previous_day = target_date - timedelta(days=1)

    # 3. 格式化为 "YYYY-MM" 字符串
    return previous_day.strftime("%Y-%m")

