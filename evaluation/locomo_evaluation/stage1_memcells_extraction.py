from typing import Dict, List
import json
import os
import sys
import uuid
import asyncio
import time
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    MofNCompleteColumn,
)
from rich.console import Console

# Ensure project root is on sys.path so `src` can be imported when running directly
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", ".."))
SRC_DIR = os.path.join(PROJECT_ROOT, "src")
SRC_DEV_DIR = os.path.join(PROJECT_ROOT, "src_dev")
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
if SRC_DIR not in sys.path:
    sys.path.insert(0, SRC_DIR)
if SRC_DEV_DIR not in sys.path:
    sys.path.insert(0, SRC_DEV_DIR)


from src.common_utils.datetime_utils import (
    to_iso_format,
    from_iso_format,
    from_timestamp,
    get_now_with_timezone,
)
from src.memory_layer.llm.llm_provider import LLMProvider
from src.memory_layer.memcell_extractor.base_memcell_extractor import RawData, MemCell
from src.memory_layer.memcell_extractor.conv_memcell_extractor import (
    ConvMemCellExtractor,
    ConversationMemCellExtractRequest,
)
from src.memory_layer.memory_extractor.episode_memory_extractor_locomo import (
    EpisodeMemoryExtractRequest,
    EpisodeMemoryExtractor,
)
from src.memory_layer.memory_extractor.event_log_extractor import EventLogExtractor
from src.memory_layer.types import RawDataType

from evaluation.locomo_evaluation.config import ExperimentConfig
from datetime import datetime, timedelta


def parse_locomo_timestamp(timestamp_str: str) -> datetime:
    """Parse LoComo timestamp format to datetime object."""
    timestamp_str = timestamp_str.replace("\\s+", " ").strip()
    dt = datetime.strptime(timestamp_str, "%I:%M %p on %d %B, %Y")
    return dt


def raw_data_load(locomo_data_path: str) -> Dict[str, List[RawData]]:
    with open(locomo_data_path, "r") as f:
        data = json.load(f)

    # data = [data[2]]
    # data = [data[0], data[1], data[2]]
    raw_data_dict = {}

    conversations = [data[i]['conversation'] for i in range(len(data))]
    print(f"   ğŸ“… Found {len(conversations)} conversations")
    for con_id, conversation in enumerate(conversations):
        messages = []
        # print(conversation.keys())
        session_keys = sorted(
            [
                key
                for key in conversation
                if key.startswith("session_") and not key.endswith("_date_time")
            ]
        )

        print(f"   ğŸ“… Found {len(session_keys)} sessions")
        print(
            f"   ğŸ­ Speakers: {conversation.get('speaker_a', 'Unknown')} & {conversation.get('speaker_b', 'Unknown')}"
        )
        speaker_name_to_id = {}
        for session_key in session_keys:
            session_messages = conversation[session_key]
            session_time_key = f"{session_key}_date_time"

            if session_time_key in conversation:
                # Parse session timestamp
                session_time = parse_locomo_timestamp(conversation[session_time_key])

                # Process each message in this session
                for i, msg in enumerate(session_messages):
                    # Generate timestamp for this message (session time + message offset)
                    msg_timestamp = session_time + timedelta(
                        seconds=i * 30
                    )  # 30 seconds between messages
                    iso_timestamp = to_iso_format(msg_timestamp)

                    # Generate unique speaker_id for this conversation
                    speaker_name = msg["speaker"]
                    if speaker_name not in speaker_name_to_id:
                        # Generate unique ID: {name}_{conversation_index}
                        unique_id = f"{speaker_name.lower().replace(' ', '_')}_{con_id}"
                        speaker_name_to_id[speaker_name] = unique_id

                    # Process content with image information if present
                    content = msg["text"]
                    if msg.get("img_url"):
                        blip_caption = msg.get("blip_caption", "an image")
                        content = f"[{speaker_name} shared an image: {blip_caption}] {content}"

                    message = {
                        "speaker_id": speaker_name_to_id[speaker_name],
                        "user_name": speaker_name,
                        "speaker_name": speaker_name,
                        "content": content,
                        "timestamp": iso_timestamp,
                        "original_timestamp": conversation[session_time_key],
                        "dia_id": msg["dia_id"],
                        "session": session_key,
                    }
                    # Add optional fields if present
                    for optional_field in ["img_url", "blip_caption", "query"]:
                        if optional_field in msg:
                            message[optional_field] = msg[optional_field]
                    messages.append(message)
        raw_data_dict[str(con_id)] = messages

        print(
            f"   âœ… Converted {len(messages)} messages from {len(session_keys)} sessions"
        )

    return raw_data_dict


def convert_conversation_to_raw_data_list(conversation: list) -> List[RawData]:
    raw_data_list = []
    for msg in conversation:
        raw_data_list.append(RawData(content=msg, data_id=str(uuid.uuid4())))
    return raw_data_list


async def memcell_extraction_from_conversation(
    raw_data_list: List[RawData],
    llm_provider: LLMProvider = None,
    memcell_extractor: ConvMemCellExtractor = None,
    smart_mask: bool = True,
    conv_id: str = None,  # æ·»åŠ ä¼šè¯IDç”¨äºè¿›åº¦æ¡æè¿°
    progress: Progress = None,  # æ·»åŠ è¿›åº¦æ¡å¯¹è±¡
    task_id: int = None,  # æ·»åŠ ä»»åŠ¡ID
) -> list:

    episode_extractor = EpisodeMemoryExtractor(llm_provider=llm_provider)
    memcell_list = []
    speakers = {
        raw_data.content["speaker_id"]
        for raw_data in raw_data_list
        if isinstance(raw_data.content, dict) and "speaker_id" in raw_data.content
    }
    history_raw_data_list = []
    # raw_data_list = raw_data_list[:100]

    # å¤„ç†æ¶ˆæ¯
    total_messages = len(raw_data_list)
    smart_mask_flag = False

    for idx, raw_data in enumerate(raw_data_list):
        # æ›´æ–°è¿›åº¦æ¡ï¼ˆåœ¨å¤„ç†å‰æ›´æ–°ï¼Œæ˜¾ç¤ºæ­£åœ¨å¤„ç†ç¬¬å‡ æ¡ï¼‰
        if progress and task_id is not None:
            progress.update(task_id, completed=idx)

        if history_raw_data_list == [] or len(history_raw_data_list) == 1:
            history_raw_data_list.append(raw_data)
            continue

        if smart_mask and len(history_raw_data_list) > 5:
            smart_mask_flag = True
            # analysis_history = history_raw_data_list[:-1]
        else:
            # analysis_history = history_raw_data_list
            smart_mask_flag = False
        request = ConversationMemCellExtractRequest(
            history_raw_data_list=history_raw_data_list,
            new_raw_data_list=[raw_data],
            user_id_list=list(speakers),
            smart_mask_flag=smart_mask_flag,
            # group_id="group_1",
        )
        for i in range(5):
            try:
                result = await memcell_extractor.extract_memcell(request)
                break
            except Exception as e:
                print('retry: ', i)
                if i == 4:
                    raise Exception("Memcell extraction failed")
                continue
        memcell_result = result[0]
        # print(f"   âœ… Memcell result: {memcell_result}")  # æ³¨é‡Šæ‰é¿å…å¹²æ‰°è¿›åº¦æ¡
        if memcell_result is None:
            history_raw_data_list.append(raw_data)
        elif isinstance(memcell_result, MemCell):
            if smart_mask_flag:
                history_raw_data_list = [history_raw_data_list[-1], raw_data]
            else:
                history_raw_data_list = [raw_data]
            memcell_result.summary = memcell_result.episode[:200] + "..."
            memcell_list.append(memcell_result)
        else:
            console = Console()
            console.print("--------------------------------")
            console.print(f"   âŒ Memcell result: {memcell_result}", style="bold red")
            raise Exception("Memcell extraction failed")

    # å¤„ç†å®Œæˆï¼Œæ›´æ–°è¿›åº¦ä¸º100%
    if progress and task_id is not None:
        progress.update(task_id, completed=total_messages)

    if history_raw_data_list:
        memcell = MemCell(
            type=RawDataType.CONVERSATION,
            event_id=str(uuid.uuid4()),
            user_id_list=list(speakers),
            original_data=history_raw_data_list,
            timestamp=(memcell_list[-1].timestamp),
            summary="111",
        )
        episode_request = EpisodeMemoryExtractRequest(
            memcell_list=[memcell],
            user_id_list=request.user_id_list,
            participants=list(speakers),
            group_id=request.group_id,
        )

        episode_result = await episode_extractor.extract_memory(
            episode_request, use_group_prompt=True
        )
        memcell.episode = episode_result.episode
        memcell.subject = episode_result.subject
        memcell.summary = episode_result.episode[:200] + "..."
        memcell.original_data = episode_extractor.get_conversation_text(
            history_raw_data_list
        )
        original_data_list = []
        for raw_data in history_raw_data_list:
            original_data_list.append(memcell_extractor._data_process(raw_data))
        memcell.original_data = original_data_list
        memcell_list.append(memcell)

    return memcell_list


async def process_single_conversation(
    conv_id: str,
    conversation: list,
    save_dir: str,
    llm_provider: LLMProvider = None,
    event_log_extractor: EventLogExtractor = None,  # æ·»åŠ event_log_extractorå‚æ•°
    progress_counter: dict = None,  # æ·»åŠ è¿›åº¦è®¡æ•°å™¨
    progress: Progress = None,  # æ·»åŠ è¿›åº¦æ¡å¯¹è±¡
    task_id: int = None,  # æ·»åŠ ä»»åŠ¡ID
) -> tuple:
    """å¤„ç†å•ä¸ªä¼šè¯å¹¶è¿”å›ç»“æœ

    Args:
        conv_id: ä¼šè¯ID
        conversation: ä¼šè¯æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        llm_provider: å…±äº«çš„LLMæä¾›è€…å®ä¾‹
        event_log_extractor: äº‹ä»¶æ—¥å¿—æå–å™¨å®ä¾‹
        progress: è¿›åº¦æ¡å¯¹è±¡
        task_id: è¿›åº¦ä»»åŠ¡ID

    Returns:
        tuple: (conv_id, memcell_list)
    """
    try:
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        if progress and task_id is not None:
            progress.update(task_id, status="å¤„ç†ä¸­")

        raw_data_list = convert_conversation_to_raw_data_list(conversation)
        memcell_extractor = ConvMemCellExtractor(llm_provider=llm_provider)
        memcell_list = await memcell_extraction_from_conversation(
            raw_data_list,
            llm_provider=llm_provider,
            memcell_extractor=memcell_extractor,
            conv_id=conv_id,  # ä¼ é€’ä¼šè¯ID
            progress=progress,  # ä¼ é€’è¿›åº¦æ¡å¯¹è±¡
            task_id=task_id,  # ä¼ é€’ä»»åŠ¡ID
        )
        # print(f"   âœ… ä¼šè¯ {conv_id}: {len(memcell_list)} memcells extracted")  # æ³¨é‡Šæ‰é¿å…å¹²æ‰°è¿›åº¦æ¡

        # åœ¨ä¿å­˜å‰è½¬æ¢æ—¶é—´æˆ³ä¸º datetime å¯¹è±¡
        for memcell in memcell_list:
            if hasattr(memcell, 'timestamp'):
                ts = memcell.timestamp
                if isinstance(ts, (int, float)):
                    # å°† int/float æ—¶é—´æˆ³è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„ datetime
                    memcell.timestamp = from_timestamp(ts)
                elif isinstance(ts, str):
                    # å°†å­—ç¬¦ä¸²æ—¶é—´æˆ³è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„ datetime
                    memcell.timestamp = from_iso_format(ts)
                elif not isinstance(ts, datetime):
                    # å¦‚æœä¸æ˜¯é¢„æœŸçš„ç±»å‹ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                    memcell.timestamp = get_now_with_timezone()

        # ğŸ”¥ ä¼˜åŒ–ï¼šå¹¶å‘ç”Ÿæˆ event logï¼ˆæå‡é€Ÿåº¦ 10-20 å€ï¼‰
        if event_log_extractor:
            # å‡†å¤‡æ‰€æœ‰éœ€è¦æå– event log çš„ memcells
            memcells_with_episode = [
                (idx, memcell) 
                for idx, memcell in enumerate(memcell_list)
                if hasattr(memcell, 'episode') and memcell.episode
            ]
            
            # å®šä¹‰å•ä¸ª event log æå–ä»»åŠ¡
            async def extract_single_event_log(idx: int, memcell):
                try:
                    event_log = await event_log_extractor.extract_event_log(
                        episode_text=memcell.episode, 
                        timestamp=memcell.timestamp
                    )
                    return idx, event_log
                except Exception as e:
                    console = Console()
                    console.print(
                        f"\nâš ï¸  ç”Ÿæˆevent logå¤±è´¥ (Conv {conv_id}, Memcell {idx}): {e}",
                        style="yellow",
                    )
                    return idx, None
            
            # ğŸ”¥ å¹¶å‘æå–æ‰€æœ‰ event logsï¼ˆä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘æ•°ï¼‰
            sem = asyncio.Semaphore(20)  # é™åˆ¶å¹¶å‘æ•°ä¸º 20ï¼ˆé¿å… API é™æµï¼‰
            
            async def extract_with_semaphore(idx, memcell):
                async with sem:
                    return await extract_single_event_log(idx, memcell)
            
            print(f"\nğŸ”¥ å¼€å§‹å¹¶å‘æå– {len(memcells_with_episode)} ä¸ª event logs...")
            event_log_tasks = [
                extract_with_semaphore(idx, memcell) 
                for idx, memcell in memcells_with_episode
            ]
            event_log_results = await asyncio.gather(*event_log_tasks)
            
            # å°† event logs å…³è”å›å¯¹åº”çš„ memcells
            for original_idx, event_log in event_log_results:
                if event_log:
                    memcell_list[original_idx].event_log = event_log
            
            print(f"âœ… Event log æå–å®Œæˆ: {sum(1 for _, el in event_log_results if el)}/{len(event_log_results)} æˆåŠŸ")

        # ä¿å­˜å•ä¸ªä¼šè¯çš„ç»“æœ
        memcell_dicts = []
        for memcell in memcell_list:
            memcell_dict = memcell.to_dict()
            # å¦‚æœæœ‰event_logï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
            if hasattr(memcell, 'event_log') and memcell.event_log:
                memcell_dict['event_log'] = memcell.event_log.to_dict()
            memcell_dicts.append(memcell_dict)

        memcell_dicts = [memcell_dict for memcell_dict in memcell_dicts]
        print(memcell_dicts)
        output_file = os.path.join(save_dir, f"memcell_list_conv_{conv_id}.json")
        with open(output_file, "w") as f:
            json.dump(memcell_dicts, f, ensure_ascii=False, indent=2)

        # æ›´æ–°è¿›åº¦ï¼ˆé™é»˜ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡ï¼‰
        if progress_counter:
            progress_counter['completed'] += 1
            # ä¸æ‰“å°ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡

        return conv_id, memcell_list

    except Exception as e:
        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘ä»¬èƒ½çŸ¥é“å…·ä½“é—®é¢˜
        console = Console()
        console.print(f"\nâŒ å¤„ç†ä¼šè¯ {conv_id} æ—¶å‡ºé”™: {e}", style="bold red")
        if progress_counter:
            progress_counter['completed'] += 1
            progress_counter['failed'] += 1
        import traceback

        traceback.print_exc()
        return conv_id, []


async def main():
    """ä¸»å‡½æ•° - å¹¶å‘å¤„ç†æ‰€æœ‰ä¼šè¯"""

    config = ExperimentConfig()
    llm_service = config.llm_service
    dataset_path = config.datase_path
    raw_data_dict = raw_data_load(dataset_path)

    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    os.makedirs(os.path.join(CURRENT_DIR, "results"), exist_ok=True)
    os.makedirs(
        os.path.join(CURRENT_DIR, "results", config.experiment_name), exist_ok=True
    )
    os.makedirs(
        os.path.join(CURRENT_DIR, "results", config.experiment_name, "memcells"),
        exist_ok=True,
    )
    save_dir = os.path.join(CURRENT_DIR, "results", config.experiment_name, "memcells")

    console = Console()
    
    # ğŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥å·²å®Œæˆçš„å¯¹è¯
    completed_convs = set()
    for conv_id in raw_data_dict.keys():
        output_file = os.path.join(save_dir, f"memcell_list_conv_{conv_id}.json")
        if os.path.exists(output_file):
            # éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§ï¼ˆéç©ºä¸”å¯è§£æï¼‰
            try:
                with open(output_file, "r") as f:
                    data = json.load(f)
                    if data and len(data) > 0:  # ç¡®ä¿æœ‰æ•°æ®
                        completed_convs.add(conv_id)
                        console.print(f"âœ… è·³è¿‡å·²å®Œæˆçš„ä¼šè¯: {conv_id} ({len(data)} memcells)", style="green")
            except Exception as e:
                console.print(f"âš ï¸  ä¼šè¯ {conv_id} æ–‡ä»¶æŸåï¼Œå°†é‡æ–°å¤„ç†: {e}", style="yellow")
    
    # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„å¯¹è¯
    pending_raw_data_dict = {
        conv_id: conv_data 
        for conv_id, conv_data in raw_data_dict.items() 
        if conv_id not in completed_convs
    }
    
    console.print(f"\nğŸ“Š æ€»å…±å‘ç° {len(raw_data_dict)} ä¸ªä¼šè¯", style="bold cyan")
    console.print(f"âœ… å·²å®Œæˆ: {len(completed_convs)} ä¸ª", style="bold green")
    console.print(f"â³ å¾…å¤„ç†: {len(pending_raw_data_dict)} ä¸ª", style="bold yellow")
    
    if len(pending_raw_data_dict) == 0:
        console.print(f"\nğŸ‰ æ‰€æœ‰ä¼šè¯å·²å®Œæˆï¼Œæ— éœ€å¤„ç†ï¼", style="bold green")
        return
    
    total_messages = sum(len(conv) for conv in pending_raw_data_dict.values())
    console.print(f"ğŸ“ å¾…å¤„ç†æ¶ˆæ¯æ•°: {total_messages}", style="bold blue")
    console.print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†å‰©ä½™ä¼šè¯...\n", style="bold green")

    # åˆ›å»ºå…±äº«çš„ LLM Provider å’Œ MemCell Extractor å®ä¾‹ï¼ˆè§£å†³è¿æ¥ç«äº‰é—®é¢˜ï¼‰
    console.print("âš™ï¸ åˆå§‹åŒ– LLM Provider...", style="yellow")
    console.print(f"   æ¨¡å‹: {config.llm_config[llm_service]['model']}", style="dim")
    console.print(
        f"   Base URL: {config.llm_config[llm_service]['base_url']}", style="dim"
    )

    shared_llm_provider = LLMProvider(
        provider_type="openai",
        model=config.llm_config[llm_service]["model"],
        api_key=config.llm_config[llm_service]["api_key"],
        base_url=config.llm_config[llm_service]["base_url"],
        temperature=config.llm_config[llm_service]["temperature"],
        max_tokens=config.llm_config[llm_service]["max_tokens"],
    )

    # åˆ›å»ºå…±äº«çš„ Event Log Extractor
    console.print("âš™ï¸ åˆå§‹åŒ– Event Log Extractor...", style="yellow")
    shared_event_log_extractor = EventLogExtractor(llm_provider=shared_llm_provider)

    # ğŸ”¥ ä½¿ç”¨å¾…å¤„ç†çš„å¯¹è¯å­—å…¸ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    # åˆ›å»ºè¿›åº¦è®¡æ•°å™¨
    progress_counter = {'total': len(pending_raw_data_dict), 'completed': 0, 'failed': 0}

    # ä½¿ç”¨ Rich è¿›åº¦æ¡
    start_time = time.time()

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),  # æ˜¾ç¤º "3/10" æ ¼å¼
        TextColumn("â€¢"),
        TaskProgressColumn(),  # æ˜¾ç¤ºç™¾åˆ†æ¯”
        TextColumn("â€¢"),
        TimeElapsedColumn(),  # å·²ç”¨æ—¶é—´
        TextColumn("â€¢"),
        TimeRemainingColumn(),  # é¢„è®¡å‰©ä½™æ—¶é—´
        TextColumn("â€¢"),
        TextColumn("[bold blue]{task.fields[status]}"),
        console=console,
        transient=False,
        refresh_per_second=1,
    ) as progress:
        # åˆ›å»ºä¸»è¿›åº¦ä»»åŠ¡
        main_task = progress.add_task(
            "[bold cyan]ğŸ¯ æ€»è¿›åº¦",
            total=len(raw_data_dict),
            completed=len(completed_convs),  # ğŸ”¥ å·²å®Œæˆçš„æ•°é‡
            status="å¤„ç†ä¸­",
        )

        # ğŸ”¥ å…ˆæ·»åŠ å·²å®Œæˆçš„ä¼šè¯åˆ°è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºä¸ºå·²å®Œæˆï¼‰
        conversation_tasks = {}
        for conv_id in completed_convs:
            conv_task_id = progress.add_task(
                f"[green]Conv-{conv_id}",
                total=len(raw_data_dict[conv_id]),
                completed=len(raw_data_dict[conv_id]),  # 100%
                status="âœ… (å·²è·³è¿‡)",
            )
            conversation_tasks[conv_id] = conv_task_id

        # ğŸ”¥ ä¸ºå¾…å¤„ç†çš„ä¼šè¯åˆ›å»ºè¿›åº¦æ¡ä»»åŠ¡
        updated_tasks = []
        for conv_id, conversation in pending_raw_data_dict.items():
            # åˆ›å»ºæ¯ä¸ªä¼šè¯çš„è¿›åº¦æ¡
            conv_task_id = progress.add_task(
                f"[yellow]Conv-{conv_id}",  # ç®€åŒ–åç§°
                total=len(conversation),  # æ¶ˆæ¯æ€»æ•°
                completed=0,  # åˆå§‹åŒ–ä¸º0
                status="ç­‰å¾…",
            )
            conversation_tasks[conv_id] = conv_task_id

            # åˆ›å»ºå¤„ç†ä»»åŠ¡
            task = process_single_conversation(
                conv_id,
                conversation,
                save_dir,
                llm_provider=shared_llm_provider,
                event_log_extractor=shared_event_log_extractor,  # ä¼ é€’event_log_extractor
                progress_counter=progress_counter,
                progress=progress,
                task_id=conv_task_id,
            )
            updated_tasks.append(task)

        # å®šä¹‰å®Œæˆæ—¶æ›´æ–°å‡½æ•°
        async def run_with_completion(task, conv_id):
            result = await task
            progress.update(
                conversation_tasks[conv_id],
                status="âœ…",
                completed=progress.tasks[conversation_tasks[conv_id]].total,
            )
            progress.update(main_task, advance=1)
            return result

        # ğŸ”¥ å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
        if updated_tasks:
            results = await asyncio.gather(
                *[
                    run_with_completion(task, conv_id)
                    for (conv_id, _), task in zip(pending_raw_data_dict.items(), updated_tasks)
                ]
            )
        else:
            results = []
        # with open(os.path.join(save_dir, "response_info.json"), "w") as f:
        #     json.dump(shared_llm_provider.provider.response_info, f, ensure_ascii=False, indent=2)
        # æ›´æ–°ä¸»è¿›åº¦ä¸ºå®Œæˆ
        progress.update(main_task, status="âœ… å®Œæˆ")

    end_time = time.time()

    # ç»Ÿè®¡ç»“æœ
    all_memcells = []
    successful_convs = 0
    for conv_id, memcell_list in results:
        if memcell_list:
            successful_convs += 1
            all_memcells.extend(memcell_list)

    console.print("\n" + "=" * 60, style="dim")
    console.print("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:", style="bold")
    console.print(
        f"   âœ… æˆåŠŸå¤„ç†ä¼šè¯æ•°: {successful_convs}/{len(raw_data_dict)}", style="green"
    )
    console.print(f"   ğŸ“ æ€»å…±æå–çš„ memcells: {len(all_memcells)}", style="blue")
    console.print(f"   â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’", style="yellow")
    console.print(
        f"   ğŸš€ å¹³å‡æ¯ä¼šè¯è€—æ—¶: {(end_time - start_time)/len(raw_data_dict):.2f} ç§’",
        style="cyan",
    )
    console.print("=" * 60, style="dim")

    # ä¿å­˜æ±‡æ€»ç»“æœ
    all_memcells_dicts = [memcell.to_dict() for memcell in all_memcells]
    summary_file = os.path.join(save_dir, "memcell_list_all.json")
    with open(summary_file, "w") as f:
        json.dump(all_memcells_dicts, f, ensure_ascii=False, indent=2)
    console.print(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}", style="green")

    # ä¿å­˜å¤„ç†æ‘˜è¦
    summary = {
        "total_conversations": len(raw_data_dict),
        "successful_conversations": successful_convs,
        "total_memcells": len(all_memcells),
        "processing_time_seconds": end_time - start_time,
        "average_time_per_conversation": (end_time - start_time) / len(raw_data_dict),
        "conversation_results": {
            conv_id: len(memcell_list) for conv_id, memcell_list in results
        },
    }
    summary_info_file = os.path.join(save_dir, "processing_summary.json")
    with open(summary_info_file, "w") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    console.print(f"ğŸ“Š å¤„ç†æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_info_file}\n", style="green")


if __name__ == "__main__":
    asyncio.run(main())
